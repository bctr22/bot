{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQX1ok0H4WpjDT8DYxYME7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hl2aUyqjdbRp","executionInfo":{"status":"ok","timestamp":1712945038032,"user_tz":-420,"elapsed":84974,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}},"outputId":"9a803cad-9ea6-4470-ecbf-ff3544a1f25f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DGA'...\n","remote: Enumerating objects: 401, done.\u001b[K\n","remote: Total 401 (delta 0), reused 0 (delta 0), pack-reused 401\u001b[K\n","Receiving objects: 100% (401/401), 85.54 MiB | 9.41 MiB/s, done.\n","Resolving deltas: 100% (216/216), done.\n","Updating files: 100% (46/46), done.\n","Collecting wordsegment\n","  Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wordsegment\n","Successfully installed wordsegment-1.3.1\n","Collecting pyenchant\n","  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyenchant\n","Successfully installed pyenchant-3.2.2\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  dictionaries-common hunspell-en-us libhunspell-1.7-0 libtext-iconv-perl\n","Suggested packages:\n","  wordlist openoffice.org-hunspell | openoffice.org-core\n","The following NEW packages will be installed:\n","  dictionaries-common hunspell hunspell-en-us libhunspell-1.7-0 libtext-iconv-perl\n","0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 723 kB of archives.\n","After this operation, 2,412 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 hunspell amd64 1.7.0-4build1 [67.9 kB]\n","Fetched 723 kB in 1s (607 kB/s)\n","Preconfiguring packages ...\n","Selecting previously unselected package libtext-iconv-perl.\n","(Reading database ... 121752 files and directories currently installed.)\n","Preparing to unpack .../libtext-iconv-perl_1.7-7build3_amd64.deb ...\n","Unpacking libtext-iconv-perl (1.7-7build3) ...\n","Selecting previously unselected package dictionaries-common.\n","Preparing to unpack .../dictionaries-common_1.28.14_all.deb ...\n","Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n","Unpacking dictionaries-common (1.28.14) ...\n","Selecting previously unselected package hunspell-en-us.\n","Preparing to unpack .../hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n","Unpacking hunspell-en-us (1:2020.12.07-2) ...\n","Selecting previously unselected package libhunspell-1.7-0:amd64.\n","Preparing to unpack .../libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n","Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n","Selecting previously unselected package hunspell.\n","Preparing to unpack .../hunspell_1.7.0-4build1_amd64.deb ...\n","Unpacking hunspell (1.7.0-4build1) ...\n","Setting up libtext-iconv-perl (1.7-7build3) ...\n","Setting up dictionaries-common (1.28.14) ...\n","Setting up hunspell-en-us (1:2020.12.07-2) ...\n","Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n","Setting up hunspell (1.7.0-4build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  libreoffice-writer\n","The following NEW packages will be installed:\n","  hunspell-es\n","0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 288 kB of archives.\n","After this operation, 1,442 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-es all 1:7.2.0-2 [288 kB]\n","Fetched 288 kB in 1s (350 kB/s)\n","Selecting previously unselected package hunspell-es.\n","(Reading database ... 121848 files and directories currently installed.)\n","Preparing to unpack .../hunspell-es_1%3a7.2.0-2_all.deb ...\n","Unpacking hunspell-es (1:7.2.0-2) ...\n","Setting up hunspell-es (1:7.2.0-2) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  libreoffice-writer\n","The following NEW packages will be installed:\n","  hunspell-de-de\n","0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 286 kB of archives.\n","After this operation, 1,176 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-de-de all 20161207-9 [286 kB]\n","Fetched 286 kB in 1s (348 kB/s)\n","Selecting previously unselected package hunspell-de-de.\n","(Reading database ... 121893 files and directories currently installed.)\n","Preparing to unpack .../hunspell-de-de_20161207-9_all.deb ...\n","Unpacking hunspell-de-de (20161207-9) ...\n","Setting up hunspell-de-de (20161207-9) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  hunspell-fr-classical\n","The following NEW packages will be installed:\n","  hunspell-fr hunspell-fr-classical\n","0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 350 kB of archives.\n","After this operation, 2,935 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-fr-classical all 1:7.0-1 [347 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-fr all 1:7.0-1 [2,976 B]\n","Fetched 350 kB in 1s (358 kB/s)\n","Selecting previously unselected package hunspell-fr-classical.\n","(Reading database ... 121906 files and directories currently installed.)\n","Preparing to unpack .../hunspell-fr-classical_1%3a7.0-1_all.deb ...\n","Unpacking hunspell-fr-classical (1:7.0-1) ...\n","Selecting previously unselected package hunspell-fr.\n","Preparing to unpack .../hunspell-fr_1%3a7.0-1_all.deb ...\n","Unpacking hunspell-fr (1:7.0-1) ...\n","Setting up hunspell-fr-classical (1:7.0-1) ...\n","Setting up hunspell-fr (1:7.0-1) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  libreoffice-writer\n","The following NEW packages will be installed:\n","  hunspell-it\n","0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 355 kB of archives.\n","After this operation, 1,776 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-it all 1:7.2.0-2 [355 kB]\n","Fetched 355 kB in 1s (411 kB/s)\n","Selecting previously unselected package hunspell-it.\n","(Reading database ... 121941 files and directories currently installed.)\n","Preparing to unpack .../hunspell-it_1%3a7.2.0-2_all.deb ...\n","Unpacking hunspell-it (1:7.2.0-2) ...\n","Setting up hunspell-it (1:7.2.0-2) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  libreoffice-writer\n","The following NEW packages will be installed:\n","  hunspell-uk\n","0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 1,199 kB of archives.\n","After this operation, 8,849 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-uk all 1:7.2.0-2 [1,199 kB]\n","Fetched 1,199 kB in 1s (1,667 kB/s)\n","Selecting previously unselected package hunspell-uk.\n","(Reading database ... 121948 files and directories currently installed.)\n","Preparing to unpack .../hunspell-uk_1%3a7.2.0-2_all.deb ...\n","Unpacking hunspell-uk (1:7.2.0-2) ...\n","Setting up hunspell-uk (1:7.2.0-2) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  aspell aspell-en libaspell15 libenchant-2-2\n","Suggested packages:\n","  aspell-doc spellutils libenchant-2-voikko\n","The following NEW packages will be installed:\n","  aspell aspell-en enchant-2 libaspell15 libenchant-2-2\n","0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 776 kB of archives.\n","After this operation, 3,269 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaspell15 amd64 0.60.8-4build1 [325 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell amd64 0.60.8-4build1 [87.7 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libenchant-2-2 amd64 2.3.2-1ubuntu2 [50.9 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 enchant-2 amd64 2.3.2-1ubuntu2 [13.0 kB]\n","Fetched 776 kB in 1s (1,272 kB/s)\n","Selecting previously unselected package libaspell15:amd64.\n","(Reading database ... 121953 files and directories currently installed.)\n","Preparing to unpack .../libaspell15_0.60.8-4build1_amd64.deb ...\n","Unpacking libaspell15:amd64 (0.60.8-4build1) ...\n","Selecting previously unselected package aspell.\n","Preparing to unpack .../aspell_0.60.8-4build1_amd64.deb ...\n","Unpacking aspell (0.60.8-4build1) ...\n","Selecting previously unselected package aspell-en.\n","Preparing to unpack .../aspell-en_2018.04.16-0-1_all.deb ...\n","Unpacking aspell-en (2018.04.16-0-1) ...\n","Selecting previously unselected package libenchant-2-2:amd64.\n","Preparing to unpack .../libenchant-2-2_2.3.2-1ubuntu2_amd64.deb ...\n","Unpacking libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n","Selecting previously unselected package enchant-2.\n","Preparing to unpack .../enchant-2_2.3.2-1ubuntu2_amd64.deb ...\n","Unpacking enchant-2 (2.3.2-1ubuntu2) ...\n","Setting up libaspell15:amd64 (0.60.8-4build1) ...\n","Setting up aspell (0.60.8-4build1) ...\n","Setting up libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n","Setting up aspell-en (2018.04.16-0-1) ...\n","Setting up enchant-2 (2.3.2-1ubuntu2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for dictionaries-common (1.28.14) ...\n","aspell-autobuildhash: processing: en [en-common].\n","aspell-autobuildhash: processing: en [en-variant_0].\n","aspell-autobuildhash: processing: en [en-variant_1].\n","aspell-autobuildhash: processing: en [en-variant_2].\n","aspell-autobuildhash: processing: en [en-w_accents-only].\n","aspell-autobuildhash: processing: en [en-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-variant_0].\n","aspell-autobuildhash: processing: en [en_AU-variant_1].\n","aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-variant_0].\n","aspell-autobuildhash: processing: en [en_CA-variant_1].\n","aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-variant_0].\n","aspell-autobuildhash: processing: en [en_GB-variant_1].\n","aspell-autobuildhash: processing: en [en_US-w_accents-only].\n","aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n","Collecting syllables\n","  Downloading syllables-1.0.9-py3-none-any.whl (15 kB)\n","Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n","  Downloading cmudict-1.0.23-py3-none-any.whl (939 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting importlib-metadata<7.0,>=5.1 (from syllables)\n","  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (6.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=5.1->syllables) (3.18.1)\n","Installing collected packages: importlib-metadata, cmudict, syllables\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 7.1.0\n","    Uninstalling importlib_metadata-7.1.0:\n","      Successfully uninstalled importlib_metadata-7.1.0\n","Successfully installed cmudict-1.0.23 importlib-metadata-6.11.0 syllables-1.0.9\n"]}],"source":["!git clone https://github.com/andrewaeva/DGA\n","!pip install wordsegment\n","!pip install pyenchant\n","!apt install hunspell\n","!apt install hunspell-es\n","!apt install hunspell-de-de\n","!apt install hunspell-fr\n","!apt install hunspell-it\n","!apt install hunspell-uk\n","!apt-get install enchant-2\n","!pip install syllables"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2Ua8vVpd8ho","executionInfo":{"status":"ok","timestamp":1712945483235,"user_tz":-420,"elapsed":22866,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}},"outputId":"8d2ca4c9-c58c-4318-d1ae-4c40629ea8f4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data = \"\"\"cryptolocker    1\n","zeus            2\n","pushdo          3\n","rovnix          4\n","tinba           5\n","matsnu          7\n","ramdo           8\"\"\"\n","\n","# Split the data into lines\n","lines = data.split('\\n')\n","\n","# Extract names and labels into separate lists\n","names = []\n","labels = []\n","\n","for line in lines:\n","    parts = line.split()\n","    if len(parts) == 2:\n","        name, label = parts\n","        names.append(name)\n","        labels.append(int(label))\n","\n","lines = []\n","print(len(names))\n","for i in range(len(names)):\n","    with open(\"DGA/dga_wordlists/\"+names[i]+\".txt\", \"r\") as f:\n","        lines += [next(f).strip()+' '+str(labels[i]) for _ in range(30000)]\n","\n","\n","with open(\"DGA/all_legit.txt\") as f:\n","    lines += [next(f).strip() for _ in range(300123)]\n","\n","with open(\"/content/drive/MyDrive/dataset/datset1.txt\", \"w\") as f:\n","    f.write('\\n'.join(lines))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DW9fgIuZeYy6","executionInfo":{"status":"ok","timestamp":1712946489749,"user_tz":-420,"elapsed":1169,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}},"outputId":"11e62c66-56c3-4524-d426-23f89397a258"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["7\n"]}]},{"cell_type":"code","source":["# features\n","\n","\n","\n","# domain length\n","def getDomainLength(domain):\n","    return len(domain)\n","\n","\n","from wordsegment import load, segment\n","import wordsegment\n","\n","def domain2list(domain):\n","    if (len(domain)>0):\n","        s = wordsegment.segment(domain)\n","        return s\n","    return []\n","# number of length\n","def getNumOfHyphen(domain):\n","    return domain.count(\"-\")\n","# number of numeric tokens\n","import re\n","def getNumOfDigit(domain):\n","    n = re.findall(r\"\\d+\",domain)\n","    return len(n)\n","\n","# rare of word\n","import nltk\n","from nltk.corpus import brown\n","from nltk.probability import FreqDist\n","nltk.download('brown')\n","def getRareOfWords(domain_word_list):\n","    randomWordCount = 0\n","    rareWordCount = 0\n","    commonWordCount = 0\n","    for i in domain_word_list:\n","        if domain_freq.get(i) is None:\n","            randomWordCount += 1\n","        elif domain_freq.get(i) < 10000:\n","            rareWordCount +=1\n","        else:\n","            commonWordCount +=1\n","    global rareness\n","    rareness = [randomWordCount, rareWordCount, commonWordCount]\n","    return rareness\n","\n","def loadRareness(domain_word_list):\n","    getRareOfWords(domain_word_list)\n","\n","# common ratio\n","def getCommonRatio(domain_word_list):\n","    return rareness[2] / len(domain_word_list)\n","\n","# rare ratio\n","def getRareRatio(domain_word_list):\n","    return rareness[1] / len(domain_word_list)\n","# domain type\n","def isDomainTypeIllicit(domaintype):\n","    if domaintype in [\"com\",\"org\",\"ru\",\"net\",\"biz\"]:\n","        return True\n","    return False\n","\n","import enchant\n","\n","\n","def getMeaningRatio(domain_word_list: list) -> list:\n","    mean = 0\n","    mispelled = 0\n","    for i in domain_word_list:\n","        if check_dict.check(i) or check_dict.check(i.capitalize()):\n","            mean+=1\n","        else: mispelled += 1\n","    return [mean/len(domain_word_list), mispelled/len(domain_word_list)]\n","\n","# no random\n","def getNumberRandomWord():\n","    return rareness[2]\n","\n","\n","# calculate weight value of each letter\n","\n","qwerty_keyboard = {\n","    '1': 3, '2': 3, '3': 3, '4': 3, '5': 3, '6': 3, '7': 3, '8': 3, '9': 3, '0': 3, '-':3, '_': 4,\n","    'q': 2, 'w': 2, 'e': 2, 'r': 2, 't': 2, 'y': 2, 'u': 2, 'i': 2, 'o': 2, 'p': 2,\n","    'a': 1, 's': 1, 'd': 1, 'f': 1, 'g': 1, 'h': 1, 'j': 1, 'k': 1, 'l': 1,\n","    'z': 2, 'x': 2, 'c': 2, 'v': 2, 'b': 2, 'n': 2, 'm': 2\n","}\n","def calculateWeightLetters(domain):\n","    weight = 0\n","    for i in domain:\n","        a = qwerty_keyboard.get(i)\n","        if type(a) is not int:\n","          print(domain)\n","          break\n","        weight += a\n","    return weight\n","\n","def calculateConsecutive(domain: str):\n","    points = 0\n","    consecutive_count = 0\n","    for i in range(len(domain)):\n","        if i > 0 and qwerty_keyboard.get(domain[i]) == qwerty_keyboard.get(domain[i-1]):\n","            consecutive_count += 1\n","        else:\n","            consecutive_count = 1\n","        if consecutive_count == 3:\n","            points += 1\n","\n","    return points\n","\n","\n","# digraph frequency\n","from collections import Counter\n","\n","def calculate_digraph_frequency(text):\n","    digraphs = [text[i:i+2] for i in range(len(text)-1)]\n","    return Counter(digraphs)\n","def calculate_trigraph_frequency(text):\n","    trigraphs = [text[i:i+3] for i in range(len(text)-2)]\n","    return Counter(trigraphs)\n","def calculate_char_frequency(text):\n","    chars = [text[i] for i in range(len(text))]\n","    return Counter(chars)\n","\n","# difficult of word\n","def getWeightDifficult(domain, domain_word_list):\n","    domain_len = getDomainLength(domain)\n","    w = getNumOfDigit(domain) / domain_len\n","    h = getNumOfHyphen(domain) / domain_len\n","\n","    dgraph = calculate_digraph_frequency(domain)\n","    trigraph = calculate_digraph_frequency(domain)\n","    s_dgraph = sum([value for key, value in dgraph.items()])\n","    s_trigraph = sum([value for key, value in trigraph.items()])\n","\n","    d = ( s_dgraph + s_trigraph) / domain_len\n","    list_dom = domain_word_list\n","    r = rareness[1] / len(list_dom)\n","    l = 0\n","    if domain_len > 10:\n","        l = domain_len - 10\n","    weight_letter = calculateWeightLetters(domain)\n","    y = calculateConsecutive(domain)\n","\n","    weight = w + h + d + r + l + weight_letter - y\n","    return weight\n","\n","# get readable index\n","import re\n","import syllables\n","\n","def getReadableIndex(domain_word_list):\n","    text = ' '.join(domain_word_list)\n","    flesch_reading_ease = 206.835 - 1.015 * (len(re.findall(r'\\b\\w+\\b', text))) - \\\n","                    84.6 * (sum(syllables.estimate(word) for word in re.findall(r'\\b\\w+\\b', text)) / len(re.findall(r'\\b\\w+\\b', text)))\n","    return flesch_reading_ease\n","\n","# get frequency\n","def getDCharD(domain):\n","    dgraph = calculate_digraph_frequency(domain)\n","    trigraph = calculate_digraph_frequency(domain)\n","    chars = calculate_char_frequency(domain)\n","    s_dgraph = sum([value for key, value in dgraph.items()])\n","    s_trigraph = sum([value for key, value in trigraph.items()])\n","    s_chars = sum([value for key, value in chars.items()])\n","    s = s_chars + s_trigraph + s_dgraph\n","    return s/len(domain)\n","\n","# get char frequen in alexa-top 1 mil\n","\n","freq_letter = {\n","    'a': 7.07, 'b': 2.05, 'c': 7.06, 'd': 2.67, 'e': 7.89,\n","    'f': 1.32, 'g': 2.36, 'h': 2.01, 'i': 5.71, 'j': 0.41,\n","    'k': 1.49, 'l': 3.90, 'm': 6.31, 'n': 5.05, 'o': 10.08,\n","    'p': 2.46, 'q': 0.14, 'r': 6.04, 's': 5.14, 't': 5.19,\n","    'u': 2.97, 'v': 1.11, 'w': 0.94, 'x': 0.47, 'y': 1.26,\n","    'z': 0.57, '0': 0.11, '1': 0.16, '2': 0.15, '3': 0.09,\n","    '4': 0.10, '5': 0.06, '6': 0.05, '7': 0.05, '8': 0.06,\n","    '9': 0.05, '-': 0.82, '_': 6.63\n","}\n","\n","def getCharFreq(domain):\n","    freq = 0\n","    for i in domain:\n","        freq+=freq_letter[i]\n","    return freq\n","\n","\n","# calculate entropy\n","import math\n","\n","\n","def getEntropy(domain):\n","    \"Calculates the Shannon entropy of a string\"\n","    # get probability of chars in string\n","    prob = [ float(domain.count(c)) / len(domain) for c in dict.fromkeys(list(domain)) ]\n","    # calculate the entropy\n","    entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])\n","    return entropy\n","\n","\n","\n","def Load():\n","    global words, domain_freq\n","    words = [word.lower() for word in brown.words() if word.isalpha()]\n","    domain_freq = FreqDist(words)\n","\n","    global check_dict\n","    check_dict = enchant.Dict(\"en-US\")\n","    for i in enchant.list_languages():\n","        check_dict.add(i)\n","\n","    wordsegment.load()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQH3JWU7gmC5","executionInfo":{"status":"ok","timestamp":1712946723602,"user_tz":-420,"elapsed":2621,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}},"outputId":"37a1d603-9f50-4796-a15d-f2ec1dc5e68c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"]}]},{"cell_type":"code","source":["def get_domain(domain):\n","    a = domain.split('.')\n","def getFeature(domain, label, domaintype):\n","    domain_word_list = domain2list(domain)\n","    meaning = getMeaningRatio(domain_word_list)\n","    loadRareness(domain_word_list)\n","    features = []\n","    features.append(domain)\n","    features.append(getDomainLength(domain))\n","    features.append(len(domain_word_list))\n","    features.append(getNumOfHyphen(domain))\n","    features.append(getNumOfDigit(domain))\n","    features.append(getRareRatio(domain_word_list))\n","    features.append(getCommonRatio(domain_word_list))\n","    features.append(meaning[0])\n","    features.append(meaning[1])\n","    features.append(getNumberRandomWord())\n","    features.append(getReadableIndex(domain_word_list))\n","    features.append(getWeightDifficult(domain, domain_word_list))\n","    features.append(isDomainTypeIllicit(domaintype))\n","    features.append(getDCharD(domain))\n","    features.append(getCharFreq(domain))\n","    features.append(getEntropy(domain))\n","    features.append(label)\n","\n","    return features"],"metadata":{"id":"PErm_YQvlOAs","executionInfo":{"status":"ok","timestamp":1712946736630,"user_tz":-420,"elapsed":320,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["dataset\n"],"metadata":{"id":"gsLuM0KSlSAH"}},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","main_data = pd.read_csv('/content/drive/MyDrive/dataset/datset1.txt',sep =' ', header= None)\n","main_data.columns = ['domain','type']"],"metadata":{"id":"bTBZT2d7lP5D","executionInfo":{"status":"ok","timestamp":1712946800380,"user_tz":-420,"elapsed":2401,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Feature collection"],"metadata":{"id":"EbZu1jebleoI"}},{"cell_type":"code","source":["feature_names = ['Domain','DomainLength', 'NoOfDomainWords', 'NoOfHyphen',\n","                 'NoOfNumericToken', 'RareRatio', 'CommonRatio',\n","                 'HasMeaningRatio', 'MisspelledRatio', 'NoOfRandom',\n","                 'Readable', 'isDifficult', 'Dtype', 'DCharD',\n","                 'CharFreq', 'Entropy','Label'\n","                 ]\n","\n","domain_features = []\n","Load()\n","for i in range(510122):\n","    a = main_data['domain'][i].split('.')\n","    domain = a[0]\n","    domaintype = a[1]\n","    label = main_data['type'][i]\n","    # i have changed label here. You can keep the label value or whatever you want here.\n","    if(int(label) > 1):\n","        label = 1\n","    print(\"Collecting...\", i, end=\"\\r\")\n","    domain_features.append(getFeature(domain,label,domaintype))\n","\n","domain_data = pd.DataFrame(domain_features, columns=feature_names)\n","\n","domain_data.to_csv('/content/drive/MyDrive/dataset/data0_atm.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_EZhoHmplg1B","executionInfo":{"status":"ok","timestamp":1712949011923,"user_tz":-420,"elapsed":2177071,"user":{"displayName":"Lê Minh","userId":"07009627646018509546"}},"outputId":"f9b1ee4b-be1c-42e7-81b4-c5d9ca612593"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"xE-RFDsTm1iJ"}},{"cell_type":"code","source":["data = domain_data.drop(['Domain'],axis=1).copy()\n","data = data.sample(frac=1).reset_index(drop=True)\n","y = data['Label']\n","X = data.drop('Label',axis=1)"],"metadata":{"id":"83k2shWim1Ap"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decision tree"],"metadata":{"id":"wEW1F1x_mPWA"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size = 0.36, random_state = 12)\n"],"metadata":{"id":"d5RITrZemP-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","tree = DecisionTreeClassifier(max_depth=5)\n","tree.fit(X_train, y_train)\n","\n","y_test_tree = tree.predict(X_test)\n","\n","acc_test_tree = accuracy_score(y_test,y_test_tree)\n","\n","print(\"Decision Tree: Accuracy on test Data: {}\".format(acc_test_tree))\n","conf_matrix = confusion_matrix(y_test, y_test_tree)\n","\n","tn, fp, fn, tp = conf_matrix.ravel()\n","\n","# Calculate precision, recall, and F1 score\n","precision = precision_score(y_test, y_test_tree)\n","recall = recall_score(y_test, y_test_tree)\n","f1 = f1_score(y_test, y_test_tree)\n","print(\"False Positives: {}\".format(fp))\n","print(\"False Negatives: {}\".format(fn))\n","print(\"Precision: {}\".format(precision))\n","print(\"Recall: {}\".format(recall))\n","print(\"F1 Score: {}\".format(f1))"],"metadata":{"id":"FvbcRXuWmeQ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["k-NN algorithm"],"metadata":{"id":"NN8QIM52mkbL"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# K-Nearest Neighbors\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","\n","y_test_knn = knn.predict(X_test)\n","\n","acc_test_knn = accuracy_score(y_test, y_test_knn)\n","\n","print(\"K-Nearest Neighbors: Accuracy on test data: {}\".format(acc_test_knn))\n","conf_matrix = confusion_matrix(y_test, y_test_tree)\n","\n","tn, fp, fn, tp = conf_matrix.ravel()\n","\n","# Calculate precision, recall, and F1 score\n","precision = precision_score(y_test, y_test_tree)\n","recall = recall_score(y_test, y_test_tree)\n","f1 = f1_score(y_test, y_test_tree)\n","print(\"False Positives: {}\".format(fp))\n","print(\"False Negatives: {}\".format(fn))\n","print(\"Precision: {}\".format(precision))\n","print(\"Recall: {}\".format(recall))\n","print(\"F1 Score: {}\".format(f1))"],"metadata":{"id":"vd4N5Kb8mhn9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ANN"],"metadata":{"id":"6hSOnkdMmork"}},{"cell_type":"code","source":["# Artificial Neural Network\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","\n","ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n","ann.fit(X_train, y_train)\n","\n","y_test_ann = ann.predict(X_test)\n","\n","acc_test_ann = accuracy_score(y_test, y_test_ann)\n","\n","print(\"Artificial Neural Network: Accuracy on test data: {:.3f}\".format(acc_test_ann))\n","conf_matrix = confusion_matrix(y_test, y_test_tree)\n","\n","tn, fp, fn, tp = conf_matrix.ravel()\n","\n","# Calculate precision, recall, and F1 score\n","precision = precision_score(y_test, y_test_tree)\n","recall = recall_score(y_test, y_test_tree)\n","f1 = f1_score(y_test, y_test_tree)\n","print(\"False Positives: {}\".format(fp))\n","print(\"False Negatives: {}\".format(fn))\n","print(\"Precision: {}\".format(precision))\n","print(\"Recall: {}\".format(recall))\n","print(\"F1 Score: {}\".format(f1))"],"metadata":{"id":"TuzSpnXSmmum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SVM algorithm"],"metadata":{"id":"AuBdX6w_nD3F"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","svm = SVC(kernel='poly')\n","svm.fit(X_train, y_train)\n","\n","y_test_svm = svm.predict(X_test)\n","\n","acc_test_svm = accuracy_score(y_test, y_test_svm)\n","\n","print(\"Support Vector Machine: Accuracy on test data: {:.3f}\".format(acc_test_svm))\n","conf_matrix = confusion_matrix(y_test, y_test_tree)\n","\n","tn, fp, fn, tp = conf_matrix.ravel()\n","\n","# Calculate precision, recall, and F1 score\n","precision = precision_score(y_test, y_test_tree)\n","recall = recall_score(y_test, y_test_tree)\n","f1 = f1_score(y_test, y_test_tree)\n","print(\"False Positives: {}\".format(fp))\n","print(\"False Negatives: {}\".format(fn))\n","print(\"Precision: {}\".format(precision))\n","print(\"Recall: {}\".format(recall))\n","print(\"F1 Score: {}\".format(f1))\n"],"metadata":{"id":"9tH99ahBnBbJ"},"execution_count":null,"outputs":[]}]}